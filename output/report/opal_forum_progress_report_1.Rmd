---
title: 'OPAL forum: progress report 1'
author: "Jakub Kuzilek"
date: "25/02/2020"
output: word_document
---

```{r setup, include=FALSE}
 knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
library(magrittr)
library(readr)
library(tidytext)
library(stopwords)
library(SnowballC) # for stemming or hunspell maybe
library(ggplot2)
library(wordcloud)
library(tm)
library(topicmodels)
library(plotly)
library(RColorBrewer)
```

```{r, include=FALSE}
forum_text <- 
  read_csv("../../output/data/forum_texts.csv")

stop_words_de <-
  read_lines("../../data/stopwords-de.txt") %>% 
  tibble(word = .)

forum_words <- 
  forum_text %>% 
  unnest_tokens(word, text) %>%  # tokenization
  anti_join(stop_words_de) %>% # stop words
  anti_join(stop_words) %>% 
  filter(!str_detect(word, "\\d+")) %>% # remove single numbers
  # mutate(word = wordStem(word,language = "de")) %>% # stemming
  {.}

top_5_forums <- 
  forum_text %>% 
  count(forum_id, sort = TRUE) %>% 
  top_n(5,n) %>% 
  extract2("forum_id")
```


## General information

* Set the repository at DFKI gitlab. Milos K., Xia W. and Jakub K. have the access. 
* Additional correction of texts for the sentiment analysis.
* Focus on top 5 forums (in sense of number of messages) `top_5_forums`
* Analysis done:
  * Word and document frequencies
  * Word clouds
  * Sentiment analysis
* Analysis to be done:
  * Correlation analysis between sentiments and emojis
  * Topic modeling
  * Aditional analysis of sentiments 
  

## Data processing

* using R with packages for text mining 
* texts of the messages were:
  1. tokenized
  2. removed stop words using (https://github.com/stopwords-iso/stopwords-de/blob/master/stopwords-de.txt)
  3. removed also english stopwords (using stopwords available in tidytext R package)
  4. removed single numbers
* under consideration: stemming, removing words shorter than 3 characters

## Term frequency
Top 15 words in sense of term frequency:
```{r}
forum_words %>% 
  filter(forum_id %in% top_5_forums) %>% 
  count(word, sort = TRUE) %>%
  top_n(15,n) %>% 
  mutate(word = reorder(word,n)) %>% 
  ggplot(aes(word,n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

Top 15 words in sense of term frequency for each forum:
```{r, fig.height=7}
forum_words %>% 
  filter(forum_id %in% top_5_forums) %>% 
  count(forum_id, word, sort = TRUE) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(forum_id) %>% 
  top_n(15,n) %>% 
  ggplot(aes(word, n, fill = as.factor(forum_id))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Number of words in forum") +
  facet_wrap(~forum_id, ncol = 2, scales = "free") +
  coord_flip()
```

## Term frequency - inverse document frequency (TF-IDF)
The TF-IDF shows the "most relevant" terms in text.

Top 15 most relevant terms in all 5 forums:
```{r}
forum_words %>% 
  count(forum_id, word) %>% 
  filter(forum_id %in% top_5_forums) %>% 
  bind_tf_idf(word, forum_id, n) %>% 
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  top_n(15,tf_idf) %>% 
  ggplot(aes(word, tf_idf)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()
```

Top 15 most relevant terms for each forum:
```{r, fig.height=7}
forum_words %>% 
  filter(forum_id %in% top_5_forums) %>% 
  count(forum_id, word) %>% 
  bind_tf_idf(word, forum_id, n) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(forum_id) %>% 
  top_n(15,tf_idf) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = as.factor(forum_id))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~forum_id, ncol = 2, scales = "free") +
  coord_flip()
```

## Wordcloud
Wordcloud for all forums:
```{r, fig.height=6, fig.width=6}
forum_words %>% 
  filter(forum_id %in% top_5_forums) %>% 
  count(word) %>% 
  with(wordcloud(word, 
                 n,
                 max.words = 200,
                 colors = brewer.pal(8,"Dark2"),
                 random.order = FALSE,
                 rot.per = 0.35))
```

\newpage
## Sentiment analysis
* For the analysis the sentiments corpus from Leipzig university has been used (https://wortschatz.uni-leipzig.de/de/download).

```{r,include=FALSE}
sentiments_de <- 
  read_csv("../data/sentiments_de.csv")
```

Sentiment for each forum:
```{r}
forum_words %>% 
  filter(forum_id %in% top_5_forums) %>%
  left_join(sentiments_de, by = "word") %>% 
  mutate(value = replace_na(value, 0)) %>% 
  group_by(forum_id, message_id) %>% 
  summarise(total_sentiment = sum(value)) %>% 
  ungroup() %>% 
  group_by(forum_id) %>% 
  summarise(`average sentiment` = mean(total_sentiment),
            `total sentiment` = sum(total_sentiment)) %>%
  arrange(desc(`average sentiment`)) %>% 
  rename(`forum id` = forum_id) %>% 
  knitr::kable()
```

\newpage
Sentiment "trajectory" for each forum:

```{r, fig.height=7}
forum_words %>% 
  filter(forum_id %in% top_5_forums) %>%
  left_join(sentiments_de, by = "word") %>% 
  mutate(value = replace_na(value, 0)) %>% 
  group_by(forum_id, message_id) %>% 
  summarise(total_sentiment = sum(value)) %>% 
  ungroup() %>% 
  group_by(forum_id) %>% #summarise(n = mean(total_sentiment)) %>% arrange(desc(n)) 
  arrange(message_id) %>%
  mutate(message_id = row_number()) %>% 
  ungroup() %>% 
  ggplot(aes(message_id, total_sentiment, fill = as.factor(forum_id))) +
  geom_col(show.legend = FALSE) +
  labs(x = "order of the message", y = "message sentiment") +
  scale_x_continuous(expand = c(0, 0)) +
  facet_wrap(~forum_id, ncol = 1, scales = "free_x") 
```
